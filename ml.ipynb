{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random linear data with noise\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train = X[:80]\n",
    "X_test = X[80:]\n",
    "y_train = y[:80] \n",
    "y_test = y[80:]\n",
    "\n",
    "#plotting the data\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training and Test Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                         n_features=20,\n",
    "                         n_classes=2,\n",
    "                         n_clusters_per_class=2,\n",
    "                         n_redundant=2,\n",
    "                         n_informative=10,\n",
    "                         random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec386b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ac01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import minst\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class MyFavoriteLinearregressor:\n",
    "    def __init__(self, model_name: Literal[\"Linear\",\"Ridge\",\"Lasso\",\"ElasticNet\",\"SGD\"]):\n",
    "        self.model_name: Literal[\"Linear\",\"Ridge\",\"Lasso\",\"ElasticNet\",\"SGD\"]= model_name\n",
    "        self.param_grid = None\n",
    "        if model_name == 'Linear':\n",
    "            self.model = LinearRegression()\n",
    "        elif model_name == 'Ridge':\n",
    "            self.model = Ridge()\n",
    "        elif model_name == 'Lasso':\n",
    "            self.model = Lasso()\n",
    "        elif model_name == 'ElasticNet':\n",
    "            self.model = ElasticNet()\n",
    "        elif model_name == 'SGD':\n",
    "            self.model = SGDRegressor()\n",
    "        else:\n",
    "            raise ValueError('Invalid model name')\n",
    "        \n",
    "    \n",
    "    def hyperparameter_tuning(self, X, y):\n",
    "        if self.model_name == 'Linear':\n",
    "            self.param_grid = {'fit_intercept': [True, False]}\n",
    "        elif self.model_name == 'Ridge':\n",
    "            self.param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "        elif self.model_name == 'Lasso':\n",
    "            self.param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "        elif self.model_name == 'ElasticNet':\n",
    "            self.param_grid = {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
    "        elif self.model_name == 'SGD':\n",
    "            self.param_grid = {'alpha': [0.0001, 0.001, 0.01], 'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
    "        else:\n",
    "            raise ValueError('Invalid model name')\n",
    "        # Create grid search object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit grid search\n",
    "        grid_search.fit(X, y)\n",
    "    \n",
    "        # Print best parameters and score\n",
    "        print(f'Best parameters: {grid_search.best_params_}')\n",
    "        print(f'Best MSE: {-grid_search.best_score_}')\n",
    "        \n",
    "        # Update model with best estimator\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        return self    \n",
    "    def fit(self, X, y):\n",
    "        self.hyperparameter_tuning(X, y)\n",
    "        # Fit the model\n",
    "        self.model.fit(X, y)\n",
    "        y_pred = self.model.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        print(f'MSE: {mse}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def plot_learning_curve(self, X, y):\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            self.model, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"Learning Curve for {self.model_name}\")\n",
    "        plt.xlabel(\"Training examples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid()\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef163e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFavoriteBinaryClassifier:\n",
    "    def __init__(self, model_name: Literal[\"Logistic\",\"SVC\",\"RandomForest\",\"GradientBoosting\",\"XGBoost\"]):\n",
    "        self.model_name: Literal[\"Logistic\",\"SVC\",\"RandomForest\",\"GradientBoosting\",\"XGBoost\"]= model_name\n",
    "        self.param_grid = None\n",
    "        if model_name == 'Logistic':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            self.model = LogisticRegression()\n",
    "        elif model_name == 'SVC':\n",
    "            from sklearn.svm import SVC\n",
    "            self.model = SVC()\n",
    "        elif model_name == 'RandomForest':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            self.model = RandomForestClassifier()\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            self.model = GradientBoostingClassifier()\n",
    "        elif model_name == 'XGBoost':\n",
    "            from xgboost import XGBClassifier\n",
    "\n",
    "            self.model = XGBClassifier()\n",
    "        else:\n",
    "            raise ValueError('Invalid model name')\n",
    "    \n",
    "    def hyperparameter_tuning(self, X, y):\n",
    "        if self.model_name == 'Logistic':\n",
    "            self.param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            }\n",
    "        elif self.model_name == 'SVC':\n",
    "            self.param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "        elif self.model_name == 'RandomForest':\n",
    "            self.param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [None, 5, 10],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        elif self.model_name == 'GradientBoosting':\n",
    "            self.param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                'max_depth': [3, 5, 7]\n",
    "            }\n",
    "        elif self.model_name == 'XGBoost':\n",
    "            self.param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                'max_depth': [3, 5, 7]\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError('Invalid model name')\n",
    "        \n",
    "        # Create grid search object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit grid search\n",
    "        grid_search.fit(X, y)\n",
    "       \n",
    "       #print best parameters and score\n",
    "        print(f'Best parameters: {grid_search.best_params_}')\n",
    "        print(f'Best accuracy: {grid_search.best_score_}')\n",
    "        \n",
    "        # Update model with best estimator\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        self.hyperparameter_tuning(X, y)\n",
    "        # Fit the model\n",
    "        self.model.fit(X, y)\n",
    "        y_pred = self.model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def plot_precision_recall_curve(self, X, y):\n",
    "        from sklearn.metrics import precision_recall_curve\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        from sklearn.metrics import PrecisionRecallDisplay\n",
    "        import matplotlib.pyplot as plt\n",
    "        y_scores = self.model.predict_proba(X)[:, 1]\n",
    "        average_precision = average_precision_score(y, y_scores)\n",
    "        disp = PrecisionRecallDisplay.from_predictions(y, y_scores, name=self.model_name)\n",
    "        _ = disp.ax_.set_title(f\"2-class Precision-Recall curve: \"\n",
    "                               f\"AP={average_precision:0.2f}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29956b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in ['Linear','Ridge','Lasso','ElasticNet','SGD']:\n",
    "    print(f\"Testing {reg} regression\")\n",
    "    try:\n",
    "        flr = MyFavoriteLinearregressor(reg)\n",
    "    except ValueError as v:\n",
    "        print(v)\n",
    "    except Exception as e:\n",
    "        print(\"something else went wrong\",e,sep=\"\\n\\n\")\n",
    "    else:\n",
    "        flr.fit(X_train, y_train)\n",
    "        flr.plot_learning_curve(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc43cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bc in ['Logistic','SVC','RandomForest','GradientBoosting','XGBoost']:\n",
    "    print(f\"Testing {bc} classifier\")\n",
    "    try:\n",
    "        fbc = MyFavoriteBinaryClassifier(bc)\n",
    "    except ValueError as v:\n",
    "        print(v)\n",
    "    except Exception as e:\n",
    "        print(\"something else went wrong\",e,sep=\"\\n\\n\")\n",
    "    else:\n",
    "        fbc.fit(X_train, y_train)\n",
    "        try:\n",
    "            fbc.plot_precision_recall_curve(X_test, y_test)\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d24c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
